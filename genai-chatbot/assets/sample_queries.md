# ðŸ§  Sample Queries for GenAI Chatbot

These prompts help test the botâ€™s ability to retrieve relevant info, generate accurate responses, and evaluate quality using Arize AI. Try mixing domains and formats to stress-test the model!

---

## ðŸ” Knowledge-Based Retrieval

- What is Retrieval-Augmented Generation and how does it differ from vanilla LLMs?
- Summarize the latest release notes from LangChain.
- How does FAISS index documents and retrieve vectors?
- What are embedding models, and how do they improve semantic search?

---

## ðŸ› ï¸ Technical How-Tos

- How do I deploy a GenAI chatbot using Streamlit?
- Steps to connect Groq API with LangChain.
- What's the best chunking strategy for short-form text in RAG pipelines?
- How do I evaluate GenAI responses using Arize AI?

---

## ðŸ§ª Prompt Engineering

- Show a few-shot prompt template for a customer support chatbot.
- Create a prompt that guides a model to debug Python code step-by-step.
- What are best practices for constructing retrieval-aware prompts?

---

## ðŸ“Š Evaluation & Metrics

- What metrics does Arize AI use to assess GenAI output?
- How can I track hallucination frequency in LLM responses?
- What role does latency play in LLM performance?

---

## ðŸ’¬ Conversational Queries

- Recommend a good vector database for academic research.
- Why is Groq faster than OpenAI's API?
- How can I reduce memory usage in my chatbot without sacrificing quality?
- Explain RAG using a pizza-making analogy ðŸ•

---

## ðŸ§  Bonus: Stress Testing

- Whatâ€™s the relationship between cosine similarity and dot product in embeddings?
- Give me 3 advantages of using Chroma over FAISS.
- Retrieve docs with â€˜Llama 3â€™ and generate a comparative review with GPT-4.

---

> âœ… Tip: Add retrieval-rich documents to your data folder to improve context-based answers!
