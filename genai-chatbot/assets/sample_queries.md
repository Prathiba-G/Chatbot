# 🧠 Sample Queries for GenAI Chatbot

These prompts help test the bot’s ability to retrieve relevant info, generate accurate responses, and evaluate quality using Arize AI. Try mixing domains and formats to stress-test the model!

---

## 🔍 Knowledge-Based Retrieval

- What is Retrieval-Augmented Generation and how does it differ from vanilla LLMs?
- Summarize the latest release notes from LangChain.
- How does FAISS index documents and retrieve vectors?
- What are embedding models, and how do they improve semantic search?

---

## 🛠️ Technical How-Tos

- How do I deploy a GenAI chatbot using Streamlit?
- Steps to connect Groq API with LangChain.
- What's the best chunking strategy for short-form text in RAG pipelines?
- How do I evaluate GenAI responses using Arize AI?

---

## 🧪 Prompt Engineering

- Show a few-shot prompt template for a customer support chatbot.
- Create a prompt that guides a model to debug Python code step-by-step.
- What are best practices for constructing retrieval-aware prompts?

---

## 📊 Evaluation & Metrics

- What metrics does Arize AI use to assess GenAI output?
- How can I track hallucination frequency in LLM responses?
- What role does latency play in LLM performance?

---

## 💬 Conversational Queries

- Recommend a good vector database for academic research.
- Why is Groq faster than OpenAI's API?
- How can I reduce memory usage in my chatbot without sacrificing quality?
- Explain RAG using a pizza-making analogy 🍕

---

## 🧠 Bonus: Stress Testing

- What’s the relationship between cosine similarity and dot product in embeddings?
- Give me 3 advantages of using Chroma over FAISS.
- Retrieve docs with ‘Llama 3’ and generate a comparative review with GPT-4.

---

> ✅ Tip: Add retrieval-rich documents to your data folder to improve context-based answers!
